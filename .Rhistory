library(rvest)
library(stringr)
library(tibble)
library(rvest)
library(stringr)
library(tibble)
library(rvest)
library(stringr)
library(tibble)
library(xml2)
url1_companies <- html_nodes(page1, xpath= "//a[contains(@href, '/search?page=1&pr[]')]/text()")
url1_companies
#There's already a page with all of the projects listed by alphabetical order, with links to each page.
url <- "http://partnerships.ifpma.org/partnerships/by-letter/all"
dir_page <- read_html(url)
#Use CSS selectors to create list of all the URLs for each project page that we will scrape.
dir_urls <- html_nodes(dir_page,"#results-list a") %>%
html_attr("href") %>%
url_absolute(url)
#Now create a list of the program names
dir_names <- html_nodes(dir_page,"#results-list a") %>%
html_text()
#The first 8 programs are not company-led initiatives, so let's filter those out.
#I also happen to know that the WIPO RE:Search project page isn't loading correctly, so let's take that one out as well.
dir_urls2 <- dir_urls[c(9:421, 423:425)]
dir_names2 <- dir_names[c(9:421, 423:425)]
#Finally, clean up the program names:
dir_names2 <- str_trim(dir_names2, side = "both")
#Now, we can create a table to store all of the scraped text using the `dir_namse2` entries for the different rows.
#Also, we can go ahead and create a column to store the program descriptions.
length(dir_names2)
xtext <- seq(1:100)
x
xtest <- xtext
rm xtext
rm xtext
rm(xtext)
library(tibble)
class(xtest)
tibble(xtest)
x<- tibble(xtest)
class(x)
rm(c(x, xtest))
rm(c(x, xtest))
rm(x)
rm(xtest)
library(rvest)
library(stringr)
library(tibble)
library(xml2)
url <- "http://partnerships.ifpma.org/partnerships/by-letter/all"
dir_page <- read_html(url)
dir_urls <- html_nodes(dir_page,"#results-list a") %>%
html_attr("href") %>%
url_absolute(url)
#Now create a list of the program names
dir_names <- html_nodes(dir_page,"#results-list a") %>%
html_text()
prog_url <- dir_urls[c(9:421, 423:425)]
prog_name <- dir_names[c(9:421, 423:425)]
#Finally, clean up the program names:
prog_name <- str_trim(prog_name, side = "both")
###here we will make a shorter test version of both of these objects
prog_url <- prog_url[1:5]
prog_name <- prog_name[1:5]
alltext <- tibble(prog_name)
col_name <- c("Prog_dscr", "Company", "Partner", "PArtner_type", "Therapeutic_focus", "Disease", "Prog_type", "Target_pop", "Region", "Country", "Date_start", "Date_end")
col_selector <- c("#article-content p", "//a[contains(@href, '/search?page=1&co[]')]/text()", "//a[contains(@href, '/search?page=1&pa[]')]/text()", "//a[contains(@href, '/search?page=1&ptf[]')]/text()", "//a[contains(@href, '/search?page=1&t[]')]/text()", "//a[contains(@href, '/search?page=1&d[]')]/text()", "//a[contains(@href, '/search?page=1&pr[]')]/text()", "//a[contains(@href, '/search?page=1&tp[]')]/text()", "//a[contains(@href, '/search?page=1&re[]')]/text()", "//a[contains(@href, '/search?page=1&c[]')]/text()", "//a[contains(@href, '/search?page=1&y[]')]/text()", "//a[contains(@href, '/search?page=1&ey[]')]/text()")
col_name <- c("Prog_dscr", "Company", "Partner")
col_selector <- c("#article-content p", "//a[contains(@href, '/search?page=1&co[]')]/text()", "//a[contains(@href, '/search?page=1&pa[]')]/text()")
col_extract <- function(col_selector){
j <- 0
for (i in 1:length(prog_url)) {
entry <- read_html(prog_url[i]) %>%
html_nodes(col_selector) %>%
html_text() %>%
str_c(collapse = "; ")
j = j + 1
alltext[j, ncol(alltext)+1] <- entry
}
return(alltext)
}
sapply(col_selector, col_extract)
for (i in 1:length(col_selector)) {
col_extract(i)
}
col_name <- c("Company", "Partner")
col_selector <- c("//a[contains(@href, '/search?page=1&co[]')]/text()", "//a[contains(@href, '/search?page=1&pa[]')]/text()")
col_extract <- function(col_selector){
j <- 0
for (i in 1:length(prog_url)) {
entry <- read_html(prog_url[i]) %>%
html_nodes(xpath = col_selector) %>%
html_text() %>%
str_c(collapse = "; ")
j = j + 1
alltext[j, ncol(alltext)+1] <- entry
}
return(alltext)
}
for (i in 1:length(col_selector)) {
col_extract(i)
}
col_name <- c("Company", "Partner")
col_selector <- c("//a[contains(@href, '/search?page=1&co[]')]/text()", "//a[contains(@href, '/search?page=1&pa[]')]/text()")
col_extract <- function(col_selector){
j <- 0
for (i in 1:length(prog_url)) {
entry <- read_html(prog_url[i]) %>%
html_nodes(xpath = col_selector) %>%
html_text() %>%
str_c(collapse = "; ")
j = j + 1
alltext[j, ncol(alltext)+1] <- entry
}
return(alltext)
}
for (i in 1:length(col_selector)) {
col_extract(i)
}
for (i in 1:length(col_selector)) {
col_extract(col_selector[i])
}
View(alltext)
for (i in 1:length(col_selector)) {
col_extract(col_selector[i])
}
ls()
library(rvest)
library(stringr)
library(tibble)
library(xml2)
install.packages(c("broom", "callr", "data.table", "dplyr", "haven", "highr", "modelr", "munsell", "pillar", "purrr", "Rcpp", "reprex", "rlang", "rmarkdown", "stringi", "stringr", "tidyr", "tidyxl", "unpivotr", "utf8"))
library(rvest)
library(rvest)
library(stringr)
library(tibble)
library(xml2)
url <- "http://partnerships.ifpma.org/partnerships/by-letter/all"
dir_page <- read_html(url)
dir_urls <- html_nodes(dir_page,"#results-list a") %>%
html_attr("href") %>%
url_absolute(url)
url <- "http://partnerships.ifpma.org/partnerships/by-letter/all"
dir_page <- read_html(url)
dir_urls <- html_nodes(dir_page,"#results-list a") %>%
html_attr("href") %>%
url_absolute(url)
#Now create a list of the program names
dir_names <- html_nodes(dir_page,"#results-list a") %>%
html_text()
prog_url <- dir_urls[c(9:421, 423:425)]
prog_name <- dir_names[c(9:421, 423:425)]
#Finally, clean up the program names:
prog_name <- str_trim(prog_name, side = "both")
###here we will make a shorter test version of both of these objects
prog_url <- prog_url[1:5]
prog_name <- prog_name[1:5]
alltext <- tibble(prog_name)
col_name <- c("Prog_dscr", "Company", "Partner", "PArtner_type", "Therapeutic_focus", "Disease", "Prog_type", "Target_pop", "Region", "Country", "Date_start", "Date_end")
col_selector <- c("#article-content p", "//a[contains(@href, '/search?page=1&co[]')]/text()", "//a[contains(@href, '/search?page=1&pa[]')]/text()", "//a[contains(@href, '/search?page=1&ptf[]')]/text()", "//a[contains(@href, '/search?page=1&t[]')]/text()", "//a[contains(@href, '/search?page=1&d[]')]/text()", "//a[contains(@href, '/search?page=1&pr[]')]/text()", "//a[contains(@href, '/search?page=1&tp[]')]/text()", "//a[contains(@href, '/search?page=1&re[]')]/text()", "//a[contains(@href, '/search?page=1&c[]')]/text()", "//a[contains(@href, '/search?page=1&y[]')]/text()", "//a[contains(@href, '/search?page=1&ey[]')]/text()")
col_name <- c("Company", "Partner")
col_selector <- c("//a[contains(@href, '/search?page=1&co[]')]/text()", "//a[contains(@href, '/search?page=1&pa[]')]/text()")
###function v.2
col_extract <- function(col_selector){
xxx <- 0
tibble(xxx)
m <- 0
for (i in 1:length(prog_url)) {
entry <- read_html(prog_url[i]) %>%
html_nodes(xpath = col_selector) %>%
html_text() %>%
str_c(collapse = "; ")
m = m + 1
xxx[m] <- entry
}
return(xxx)
}
col_extract(col_selector[1]) %>% add_column(alltext) -> alltext2
col_extract(col_selector[1]) %>% add_column(alltext) -> alltext2
alltext2 <- add_column(alltext, col_extract(col_selector[1]))
alltext2
alltext2 <- add_column(alltext, col_extract(col_selector[2]))
alltext2
alltext2 <- add_column(alltext2, col_extract(col_selector[1]))
alltext2
alltext2 <- for (i in 1:length(col_selector)) {
alltext <- add_column(alltext, col_extract(col_selector[i]))
}
alltext
length(col_selector)
alltext2 <- for (i in 1:length(col_selector)) {
alltext <- add_column(alltext, col_extract(col_selector[i]))
}
names(alltext)
names(alltext)[ncol(alltext)]
alltext2 <- for (i in 1:length(col_selector)) {
alltext[ , ncol(alltext)+1] <- col_extract(col_selector[i])
}
alltext
ls
ls
ls()
library(rvest)
library(stringr)
library(tibble)
library(xml2)
source('~/OneDrive/projects/IFPMA-db/scrape_db.R', echo=TRUE)
