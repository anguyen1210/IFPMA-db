---
title: "IFPMA Parntership Directory"
output: html_notebook
---




Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.


```{r load the relevant packages, include=FALSE}

library(rvest)
library(stringr)
library(tibble)

```

#Using the list of all program names, we can create a list of URLs for each page we will visit to scrape data from

```{r scrape main text from each page in the site, store results in a tibble, include=FALSE}

#Reading the HTML code from the website

url <- "http://partnerships.ifpma.org/partnerships/by-letter/all"

page <- read_html(url)

#Using CSS selectors to create list of URLs
dir_urls <- html_nodes(page,"#results-list a") %>%
  html_attr("href") %>%
  url_absolute(url)

#Now create a list of the program names
program_names_all <- html_nodes(page,"#results-list a") %>%
  html_text()

#The first 8 programs are not company-led initiatives, so let's filter those out.  I also happen to know that the WIPO RE:Search project page isn't loading correctly, so let's take that one out as well.

program_urls <- dir_urls[c(9:421, 423:425)]
program_name <- program_names_all[c(9:421, 423:425)]

#And to clean up the program names:
program_name <- str_trim(program_name, side = "both")

#Now, we can create a table to store all of the scraped text using the `program_name` entries for the different rows. We'll also need another column to store out program descriptions, as well as a column to store our side bar text.

program_desc <- (1:416)
sidebar <- (1:416)
alltext <- tibble(program_name, program_desc, sidebar)

#Now we can collect the text we want from each page using the `program_urls` object.  We'll do this for the program descriptions, and then we'll do it for all of the text in the sidebars of each page.
#Extract program descriptions from each page

j <- 0

for (i in 1:length(program_urls)) {
    desc_entry <- read_html(program_urls[i]) %>%
    html_nodes("#article-content p") %>%
    html_text() %>%
    str_c(collapse = "; ")

j = j + 1

alltext[j, 2] <- desc_entry

}

#Next, we'll scrape all the sidebar text and add it to the tibble.

k <- 0

for (i in 1:length(program_urls)) {
    side_entry <- read_html(program_urls[i]) %>%
    html_nodes("#article-sidebar a , .row-title") %>%
    html_text() %>%
    str_c(collapse = "; ")

k = k + 1

alltext[k, 3] <- side_entry

}

```

Now that we've scraped all the text we want, we need to parse the text into the columns and values that we're interested in.

In the `program_desc` text, we can see that there are at least two recurring question/categories that we can subset our information into:

* Partnership objectives
* What are the health needs and challenges?

Glancing over the sidebar categories on each page, we can see that the commone ones we will be interested to capture are:

* Company(ies)
* Partner(s)
* Type of Partner(s)
* Therapeutic Focus
* Disease(s)
* Program Type(s)
* Targeted Population(s)
* Region(s)
* Number of Countries
* Country(ies)
* Start Date
* Anticipated completion date
* Completed date
* More information

```{r parse `alltext` into different columns, include=FALSE}


#Before we start manipulating all of the raw data we extracted, let's make another version to work off of

alltext2 <- alltext

#NOTE: My strategy to parse this data is to convert all text to lower case, then isolate all of the category names we're interested and make those all upper case, and then just do pattern matching for each category name, including all text (which should be lower) until the next capital letter (which would be the next category, regardless of the order of the categories).

#First convert all the description and side bar text to lowercase.

alltext2$program_desc <- str_to_lower(alltext2$program_desc)
alltext2$sidebar <- str_to_lower(alltext2$sidebar)

#Next, find the category names we're interested in and make those upper case

alltext2$program_desc <- str_replace(alltext2$program_desc, "partnership objective[s]*[\\?]*|program objective[s]*[\\?]*", "OBJECTIVES")
alltext2$program_desc <- str_replace(alltext2$program_desc, "[what are the health ]*needs and challenges[\\?]*", "NEEDS")
alltext2$sidebar <- str_replace(alltext2$sidebar, "company\\(ies\\)", "COMPANY")
alltext2$sidebar <- str_replace(alltext2$sidebar, "partner\\(s\\)", "PARTNER")
alltext2$sidebar <- str_replace(alltext2$sidebar, "type of partner\\(s\\)", "PARTYPE")
alltext2$sidebar <- str_replace(alltext2$sidebar, "therapeutic focus", "FOCUS")
alltext2$sidebar <- str_replace(alltext2$sidebar, "disease\\(s\\)", "DISEASE")
alltext2$sidebar <- str_replace(alltext2$sidebar, "program type\\(s\\)", "PROGTYPE")
alltext2$sidebar <- str_replace(alltext2$sidebar, "targeted population\\(s\\)", "TARGETPOP")
alltext2$sidebar <- str_replace(alltext2$sidebar, "region\\(s\\)", "REGION")
alltext2$sidebar <- str_replace(alltext2$sidebar, "number of countries", "CNUMBER")
alltext2$sidebar <- str_replace(alltext2$sidebar, "country\\(ies\\)", "COUNTRY")
alltext2$sidebar <- str_replace(alltext2$sidebar, "start date", "START")
alltext2$sidebar <- str_replace(alltext2$sidebar, "anticipated completion date", "ANTICIPATED")
alltext2$sidebar <- str_replace(alltext2$sidebar, "completed date", "COMPLETED")
alltext2$sidebar <- str_replace(alltext2$sidebar, "more information", "MOREINFO")

#Now we can loop through all the text in our tibble to extract the pieces of information we want.

#First, in order for the pattern matching to work how we want it for the first and last terms in the string, we need to add some dummy text to the start and end.

alltext3 <- alltext2
alltext3$program_desc <- str_c("XXX", sep = "; ", alltext3$program_desc)
alltext3$program_desc <- str_c(alltext3$program_desc, sep = "; ", "XXX")
alltext3$sidebar <- str_c("XXX", sep="; ", alltext3$sidebar)
alltext3$sidebar <- str_c(alltext3$sidebar, sep = "; ", "XXX")

#Next, create the corresponding regex search patterns for all our search terms.   

OBJECTIVES <- "OBJECTIVES; (.*?) [[:upper:]]"
NEEDS <- "NEEDS; (.*?) [[:upper:]]"
COMPANY <- ".* COMPANY; (.*?) [[:upper:]].*"
PARTNER <- ".* PARTNER; (.*?) [[:upper:]].*"
PARTYPE <- ".* PARTYPE; (.*?) [[:upper:]].*"
FOCUS <- ".* FOCUS; (.*?) [[:upper:]].*"
DISEASE <- ".* DISEASE; (.*?) [[:upper:]].*"
PROGTYPE <- ".* PROGTYPE; (.*?) [[:upper:]].*"
TARGETPOP <- ".* TARGETPOP; (.*?) [[:upper:]].*"
REGION <- ".* REGION; (.*?) [[:upper:]].*"
CNUMBER <- ".* CNUMBER; (.*?) [[:upper:]].*"
COUNTRY <- ".* COUNTRY; (.*?) [[:upper:]].*"
MOREINFO <- ".* MOREINFO; (.*?) [[:upper:]].*"
START <- ".* START; (.*?) [[:upper:]].*"
ANTICIPATED <- ".* ANTICIPATED; (.*?) [[:upper:]].*"
COMPLETED <- ".* COMPLETED; (.*?) [[:upper:]].*"

#Next we create a function that takes three arguments: x = search column, y = pattern, z = column to save results to.

extract_category <- function(searchColumn, searchPattern){
  
  resultsColumn <- 1:length(searchColumn)
  
  l <- 0
  
  for (i in 1:length(searchColumn)){
    term_match <- str_match(string = searchColumn[i], pattern = searchPattern)[2]
    
    l = l + 1
    
    resultsColumn[l] <- term_match
  }
  return(resultsColumn)
}

  
#Finally, let's run our function on all the search terms to extract the information we want for our empty columns 

alltext4 <- alltext3

alltext4$objectives <- extract_category(alltext3$program_desc, OBJECTIVES)
alltext4$needs_challenges <- extract_category(alltext3$program_desc, NEEDS)
alltext4$company <- extract_category(alltext3$sidebar, COMPANY)
alltext4$partner <- extract_category(alltext3$sidebar, PARTNER)
alltext4$partner_type <- extract_category(alltext3$sidebar, PARTYPE)
alltext4$therapeutic_focus <- extract_category(alltext3$sidebar, FOCUS)
alltext4$disease <- extract_category(alltext3$sidebar, DISEASE)
alltext4$company <- extract_category(alltext3$sidebar, COMPANY)
alltext4$program_type <- extract_category(alltext3$sidebar, PROGTYPE)
alltext4$target_pop <- extract_category(alltext3$sidebar, TARGETPOP)
alltext4$company <- extract_category(alltext3$sidebar, COMPANY)
alltext4$region <- extract_category(alltext3$sidebar, REGION)
alltext4$country_number <- extract_category(alltext3$sidebar, CNUMBER)
alltext4$country <- extract_category(alltext3$sidebar, COUNTRY)
alltext4$more_info <- extract_category(alltext3$sidebar, MOREINFO)
alltext4$date_start <- extract_category(alltext3$sidebar, START)
alltext4$date_to_be_complete <- extract_category(alltext3$sidebar, ANTICIPATED)
alltext4$date_completed <- extract_category(alltext3$sidebar, COMPLETED)

```

Finally, let's add a column with all of the URLs for each page as a reference

```{r add a reference column with URL link to each initiative in the online database, include = FALSE}

alltext4$URL <- program_urls

```

Last step, export this as a .CSV file that we can use elsewhere.

```{r write to .CSV, include=FALSE}

write.csv(alltext4, file = "IFPMA_db_alltext.csv")

```

