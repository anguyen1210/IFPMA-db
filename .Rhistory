library(rvest)
library(stringr)
library(tibble)
library(rvest)
library(stringr)
library(tibble)
library(rvest)
library(stringr)
library(tibble)
library(xml2)
url1_companies <- html_nodes(page1, xpath= "//a[contains(@href, '/search?page=1&pr[]')]/text()")
url1_companies
#There's already a page with all of the projects listed by alphabetical order, with links to each page.
url <- "http://partnerships.ifpma.org/partnerships/by-letter/all"
dir_page <- read_html(url)
#Use CSS selectors to create list of all the URLs for each project page that we will scrape.
dir_urls <- html_nodes(dir_page,"#results-list a") %>%
html_attr("href") %>%
url_absolute(url)
#Now create a list of the program names
dir_names <- html_nodes(dir_page,"#results-list a") %>%
html_text()
#The first 8 programs are not company-led initiatives, so let's filter those out.
#I also happen to know that the WIPO RE:Search project page isn't loading correctly, so let's take that one out as well.
dir_urls2 <- dir_urls[c(9:421, 423:425)]
dir_names2 <- dir_names[c(9:421, 423:425)]
#Finally, clean up the program names:
dir_names2 <- str_trim(dir_names2, side = "both")
#Now, we can create a table to store all of the scraped text using the `dir_namse2` entries for the different rows.
#Also, we can go ahead and create a column to store the program descriptions.
length(dir_names2)
xtext <- seq(1:100)
x
xtest <- xtext
rm xtext
rm xtext
rm(xtext)
library(tibble)
class(xtest)
tibble(xtest)
x<- tibble(xtest)
class(x)
rm(c(x, xtest))
rm(c(x, xtest))
rm(x)
rm(xtest)
library(rvest)
library(stringr)
library(tibble)
library(xml2)
url <- "http://partnerships.ifpma.org/partnerships/by-letter/all"
dir_page <- read_html(url)
dir_urls <- html_nodes(dir_page,"#results-list a") %>%
html_attr("href") %>%
url_absolute(url)
#Now create a list of the program names
dir_names <- html_nodes(dir_page,"#results-list a") %>%
html_text()
prog_url <- dir_urls[c(9:421, 423:425)]
prog_name <- dir_names[c(9:421, 423:425)]
#Finally, clean up the program names:
prog_name <- str_trim(prog_name, side = "both")
###here we will make a shorter test version of both of these objects
prog_url <- prog_url[1:5]
prog_name <- prog_name[1:5]
alltext <- tibble(prog_name)
col_name <- c("Prog_dscr", "Company", "Partner", "PArtner_type", "Therapeutic_focus", "Disease", "Prog_type", "Target_pop", "Region", "Country", "Date_start", "Date_end")
col_selector <- c("#article-content p", "//a[contains(@href, '/search?page=1&co[]')]/text()", "//a[contains(@href, '/search?page=1&pa[]')]/text()", "//a[contains(@href, '/search?page=1&ptf[]')]/text()", "//a[contains(@href, '/search?page=1&t[]')]/text()", "//a[contains(@href, '/search?page=1&d[]')]/text()", "//a[contains(@href, '/search?page=1&pr[]')]/text()", "//a[contains(@href, '/search?page=1&tp[]')]/text()", "//a[contains(@href, '/search?page=1&re[]')]/text()", "//a[contains(@href, '/search?page=1&c[]')]/text()", "//a[contains(@href, '/search?page=1&y[]')]/text()", "//a[contains(@href, '/search?page=1&ey[]')]/text()")
col_name <- c("Prog_dscr", "Company", "Partner")
col_selector <- c("#article-content p", "//a[contains(@href, '/search?page=1&co[]')]/text()", "//a[contains(@href, '/search?page=1&pa[]')]/text()")
col_extract <- function(col_selector){
j <- 0
for (i in 1:length(prog_url)) {
entry <- read_html(prog_url[i]) %>%
html_nodes(col_selector) %>%
html_text() %>%
str_c(collapse = "; ")
j = j + 1
alltext[j, ncol(alltext)+1] <- entry
}
return(alltext)
}
sapply(col_selector, col_extract)
for (i in 1:length(col_selector)) {
col_extract(i)
}
col_name <- c("Company", "Partner")
col_selector <- c("//a[contains(@href, '/search?page=1&co[]')]/text()", "//a[contains(@href, '/search?page=1&pa[]')]/text()")
col_extract <- function(col_selector){
j <- 0
for (i in 1:length(prog_url)) {
entry <- read_html(prog_url[i]) %>%
html_nodes(xpath = col_selector) %>%
html_text() %>%
str_c(collapse = "; ")
j = j + 1
alltext[j, ncol(alltext)+1] <- entry
}
return(alltext)
}
for (i in 1:length(col_selector)) {
col_extract(i)
}
col_name <- c("Company", "Partner")
col_selector <- c("//a[contains(@href, '/search?page=1&co[]')]/text()", "//a[contains(@href, '/search?page=1&pa[]')]/text()")
col_extract <- function(col_selector){
j <- 0
for (i in 1:length(prog_url)) {
entry <- read_html(prog_url[i]) %>%
html_nodes(xpath = col_selector) %>%
html_text() %>%
str_c(collapse = "; ")
j = j + 1
alltext[j, ncol(alltext)+1] <- entry
}
return(alltext)
}
for (i in 1:length(col_selector)) {
col_extract(i)
}
for (i in 1:length(col_selector)) {
col_extract(col_selector[i])
}
View(alltext)
for (i in 1:length(col_selector)) {
col_extract(col_selector[i])
}
ls()
library(rvest)
library(stringr)
library(tibble)
library(xml2)
install.packages(c("broom", "callr", "data.table", "dplyr", "haven", "highr", "modelr", "munsell", "pillar", "purrr", "Rcpp", "reprex", "rlang", "rmarkdown", "stringi", "stringr", "tidyr", "tidyxl", "unpivotr", "utf8"))
library(rvest)
library(rvest)
library(stringr)
library(tibble)
library(xml2)
url <- "http://partnerships.ifpma.org/partnerships/by-letter/all"
dir_page <- read_html(url)
dir_urls <- html_nodes(dir_page,"#results-list a") %>%
html_attr("href") %>%
url_absolute(url)
url <- "http://partnerships.ifpma.org/partnerships/by-letter/all"
dir_page <- read_html(url)
dir_urls <- html_nodes(dir_page,"#results-list a") %>%
html_attr("href") %>%
url_absolute(url)
#Now create a list of the program names
dir_names <- html_nodes(dir_page,"#results-list a") %>%
html_text()
prog_url <- dir_urls[c(9:421, 423:425)]
prog_name <- dir_names[c(9:421, 423:425)]
#Finally, clean up the program names:
prog_name <- str_trim(prog_name, side = "both")
###here we will make a shorter test version of both of these objects
prog_url <- prog_url[1:5]
prog_name <- prog_name[1:5]
alltext <- tibble(prog_name)
col_name <- c("Prog_dscr", "Company", "Partner", "PArtner_type", "Therapeutic_focus", "Disease", "Prog_type", "Target_pop", "Region", "Country", "Date_start", "Date_end")
col_selector <- c("#article-content p", "//a[contains(@href, '/search?page=1&co[]')]/text()", "//a[contains(@href, '/search?page=1&pa[]')]/text()", "//a[contains(@href, '/search?page=1&ptf[]')]/text()", "//a[contains(@href, '/search?page=1&t[]')]/text()", "//a[contains(@href, '/search?page=1&d[]')]/text()", "//a[contains(@href, '/search?page=1&pr[]')]/text()", "//a[contains(@href, '/search?page=1&tp[]')]/text()", "//a[contains(@href, '/search?page=1&re[]')]/text()", "//a[contains(@href, '/search?page=1&c[]')]/text()", "//a[contains(@href, '/search?page=1&y[]')]/text()", "//a[contains(@href, '/search?page=1&ey[]')]/text()")
col_name <- c("Company", "Partner")
col_selector <- c("//a[contains(@href, '/search?page=1&co[]')]/text()", "//a[contains(@href, '/search?page=1&pa[]')]/text()")
###function v.2
col_extract <- function(col_selector){
xxx <- 0
tibble(xxx)
m <- 0
for (i in 1:length(prog_url)) {
entry <- read_html(prog_url[i]) %>%
html_nodes(xpath = col_selector) %>%
html_text() %>%
str_c(collapse = "; ")
m = m + 1
xxx[m] <- entry
}
return(xxx)
}
col_extract(col_selector[1]) %>% add_column(alltext) -> alltext2
col_extract(col_selector[1]) %>% add_column(alltext) -> alltext2
alltext2 <- add_column(alltext, col_extract(col_selector[1]))
alltext2
alltext2 <- add_column(alltext, col_extract(col_selector[2]))
alltext2
alltext2 <- add_column(alltext2, col_extract(col_selector[1]))
alltext2
alltext2 <- for (i in 1:length(col_selector)) {
alltext <- add_column(alltext, col_extract(col_selector[i]))
}
alltext
length(col_selector)
alltext2 <- for (i in 1:length(col_selector)) {
alltext <- add_column(alltext, col_extract(col_selector[i]))
}
names(alltext)
names(alltext)[ncol(alltext)]
alltext2 <- for (i in 1:length(col_selector)) {
alltext[ , ncol(alltext)+1] <- col_extract(col_selector[i])
}
alltext
ls
ls
ls()
library(rvest)
library(stringr)
library(tibble)
library(xml2)
source('~/OneDrive/projects/IFPMA-db/scrape_db.R', echo=TRUE)
source('~/OneDrive/projects/IFPMA-db/scrape_db.R', echo=TRUE)
View(alltext)
source('~/OneDrive/projects/IFPMA-db/scrape_db.R', echo=TRUE)
View(alltext)
View(col_selector)
View(col_name)
sidebar_cols <- sapply(col_selector, col_extract)
length(prog_url)
(length(prog_url))
seq_along(prog_url)
col_extract <- function(col_selector){
xxx <- 0
data.frame(xxx)
m <- 0
for (i in 1:seq_along(prog_url)) {
entry <- read_html(prog_url[i]) %>%
html_nodes(xpath = col_selector) %>%
html_text() %>%
str_c(collapse = "; ")
m = m + 1
xxx[m] <- entry
}
return(xxx)
}
#test
col_name <- c("Partner_type", "Disease")
col_selector <- c("//a[contains(@href, '/search?page=1&ptf[]')]/text()", "//a[contains(@href, '/search?page=1&d[]')]/text()")
col_extract <- function(col_selector){
xxx <- 0
data.frame(xxx)
m <- 0
for (i in 1:seq_along(prog_url)) {
entry <- read_html(prog_url[i]) %>%
html_nodes(xpath = col_selector) %>%
html_text() %>%
str_c(collapse = "; ")
m = m + 1
xxx[m] <- entry
}
return(xxx)
}
sidebar_cols <- sapply(col_selector, col_extract)
col_extract <- function(col_selector){
xxx <- 0
data.frame(xxx)
m <- 0
for (i in 1:length(prog_url)) {
entry <- read_html(prog_url[i]) %>%
html_nodes(xpath = col_selector) %>%
html_text() %>%
str_c(collapse = "; ")
m = m + 1
xxx[m] <- entry
}
return(xxx)
}
sidebar_cols <- sapply(col_selector, col_extract)
#test2
col_selector <- c("//a[contains(@href, '/search?page=1&ey[]')]/text()")
col_selector
#test2
prog_url <- prog_url[415:416]
prog_name <- prog_name[415:416]
col_name <- c("Partner_type", "Date_end")
col_selector <- c("//a[contains(@href, '/search?page=1&ptf[]')]/text()", "//a[contains(@href, '/search?page=1&ey[]')]/text()")
col_extract <- function(col_selector){
xxx <- 0
data.frame(xxx)
m <- 0
for (i in 1:length(prog_url)) {
entry <- read_html(prog_url[i]) %>%
html_nodes(xpath = col_selector) %>%
html_text() %>%
str_c(collapse = "; ")
m = m + 1
xxx[m] <- entry
}
return(xxx)
}
alltext <- data.frame(prog_name)
sidebar_cols <- sapply(col_selector, col_extract)
sidebar_cols <- sapply(col_selector, col_extract)
library(rvest)
library(stringr)
library(xml2)
sidebar_cols <- sapply(col_selector, col_extract)
test_url <- "http://partnerships.ifpma.org/partnership/yaroslavl-hypertension-program"
test_selector <- "//a[contains(@href, '/search?page=1&ey[]')]/text()"
read_html(test_url) %>%
html_nodes(xpath = "//a[contains(@href, '/search?page=1&ptf[]')]/text()") %>%
html_text()
read_html(test_url) %>%
html_nodes(xpath = "//a[contains(@href, '/search?page=1&ptf[]')]/text()") %>%
html_text()
read_html(test_url) %>%
html_nodes(xpath = test_selector) %>%
html_text()
read_html(test_url) %>%
html_nodes(xpath = "//a[contains(@href, '/search?page=1&ptf[]')]/text()")
read_html(test_url) %>%
html_nodes(xpath = "//a[contains(@href, '/search?page=1&ptf[]')]/text()")
read_html(test_url) %>%
html_nodes(xpath = test_selector)
read_html(test_url) %>%
html_nodes(xpath = test_selector) %>%
html_text()
if (read_html(test_url) %>%
html_nodes(xpath = test_selector) %>%
html_text() =="character(0)"){
return("no node")}{
else{
return()
}
}
if ((read_html(test_url) %>%
html_nodes(xpath = test_selector) %>%
html_text()) =="character(0)"){
return("no node")}{
else{
return()
}
}
if (read_html(test_url) %>% html_nodes(xpath = test_selector) %>% html_text() =="character(0)"){
return("no node")}{
else{
return()
}
}
if (read_html(test_url) %>% html_nodes(xpath = test_selector) %>% html_text() =="character(0)"){
return("no node")
} else {
return()
}
if (read_html(test_url) %>% html_nodes(xpath = test_selector) %>% html_text() ==0){
return("no node")
} else {
return()
}
read_html(test_url) %>%
html_nodes(xpath = test_selector) %>%
html_text()
try(read_html(test_url) %>%
html_nodes(xpath = test_selector) %>%
html_text())
tryCatch(read_html(test_url) %>%
html_nodes(xpath = test_selector) %>%
html_text(),
error = function(e){NA}
)
tryCatch(read_html(test_url) %>%
html_nodes(xpath = test_selector) %>%
html_text(),
"character(0)" = function(e){NA}
)
tryCatch(read_html(test_url) %>%
html_nodes(xpath = test_selector) %>%
html_text(),
0 = function(e){NA}
)
read_html(test_url) %>%
html_nodes(xpath = test_selector) %>%
html_text() %>%
ifelse(identical(., character(0)), NA, .)
read_html(test_url) %>%
html_nodes(xpath = test_selector) %>%
html_text() %>%
ifelse(identical(., character(0)), NA, .) %>%
str_c(collapse = "; ")
read_html(test_url) %>%
html_nodes(xpath = "//a[contains(@href, '/search?page=1&co[]')]/text()") %>%
html_text() %>%
ifelse(identical(., character(0)), NA, .) %>%
str_c(collapse = "; ")
col_extract <- function(col_selector){
xxx <- 0
data.frame(xxx)
m <- 0
for (i in 1:length(prog_url)) {
entry <- read_html(prog_url[i]) %>%
html_nodes(xpath = col_selector) %>%
html_text() %>%
ifelse(identical(., character(0)), NA, .) %>%
str_c(collapse = "; ")
m = m + 1
xxx[m] <- entry
}
return(xxx)
}
#test2
prog_url <- prog_url[415:416]
prog_name <- prog_name[415:416]
col_name <- c("Partner_type", "Date_end")
col_selector <- c("//a[contains(@href, '/search?page=1&ptf[]')]/text()", "//a[contains(@href, '/search?page=1&ey[]')]/text()")
col_extract <- function(col_selector){
xxx <- 0
data.frame(xxx)
m <- 0
for (i in 1:length(prog_url)) {
entry <- read_html(prog_url[i]) %>%
html_nodes(xpath = col_selector) %>%
html_text() %>%
ifelse(identical(., character(0)), NA, .) %>%
str_c(collapse = "; ")
m = m + 1
xxx[m] <- entry
}
return(xxx)
}
alltext <- data.frame(prog_name)
sidebar_cols <- sapply(col_selector, col_extract)
sidebar_cols <- sapply(col_selector, col_extract)
read_html(test_url) %>%
html_nodes(xpath = "//a[contains(@href, '/search?page=1&co[]')]/text()") %>%
html_text() %>%
ifelse(identical(., character(0)), NA, .) %>%
str_c(collapse = "; ")
read_html(test_url) %>%
html_nodes(xpath = test_selector) %>%
html_text() %>%
ifelse(identical(., character(0)), NA, .) %>%
str_c(collapse = "; ")
#test2
prog_url <- prog_url[415:416]
prog_name <- prog_name[415:416]
col_name <- c("Partner_type", "Date_end")
col_selector <- c("//a[contains(@href, '/search?page=1&ptf[]')]/text()", "//a[contains(@href, '/search?page=1&ey[]')]/text()")
col_extract <- function(col_selector){
xxx <- 0
data.frame(xxx)
m <- 0
for (i in 1:length(prog_url)) {
entry <- read_html(prog_url[i]) %>%
html_nodes(xpath = col_selector) %>%
html_text() %>%
ifelse(identical(., character(0)), NA, .) %>%
str_c(collapse = "; ")
m = m + 1
xxx[m] <- entry
}
return(xxx)
}
alltext <- data.frame(prog_name)
sidebar_cols <- sapply(col_selector, col_extract)
prog_url[1]
prog_url[2]
prog_url <- html_nodes(index_page,"#results-list a") %>%
html_attr("href") %>%
url_absolute(url)
url <- "http://partnerships.ifpma.org/partnerships/by-letter/all"
index_page <- read_html(url)
prog_url <- html_nodes(index_page,"#results-list a") %>%
html_attr("href") %>%
url_absolute(url)
prog_url <- prog_url[c(9:421, 423:425)]
#test2
prog_url <- prog_url[415:416]
sidebar_cols <- sapply(col_selector, col_extract)
alltext <- cbind(alltext, sidebar_cols)
colnames(alltext)[2:ncol(alltext)] <- col_name
View(alltext)
source('~/OneDrive/projects/IFPMA-db/scrape_db.R', echo=TRUE)
source('~/OneDrive/projects/IFPMA-db/scrape_db.R', echo=TRUE)
View(alltext)
install.packages("writexl")
writexl::write.xlsx(alltext, file = "data/IFPMA_db.xlsx")
write.xlsx(alltext, file = "data/IFPMA_db.xlsx")
library(writexl)
write.xlsx(alltext, file = "data/IFPMA_db.xlsx")
#save as Excel file
writexl::write_xlsx(alltext, file = "data/IFPMA_db.xlsx")
#save as Excel file
writexl::write_xlsx(alltext, "data/IFPMA_db.xlsx")
#save as Excel file
writexl::write_xlsx(alltext, "./data/IFPMA_db.xlsx")
rm sidebar_cols
rm(sidebar_cols)
